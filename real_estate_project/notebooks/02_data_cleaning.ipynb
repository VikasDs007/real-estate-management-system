{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60f26dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Loading data for cleaning...\n",
      "âœ… Active: 1,000 rows Ã— 28 cols\n",
      "âœ… Archive: 1,000 rows Ã— 16 cols\n",
      "âœ… Clients: 1,000 rows Ã— 7 cols\n",
      "\n",
      "ğŸ¯ Ready for data cleaning!\n",
      "ğŸ“Š Total records to process: 3,000\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Setup for Data Cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# File path (same as exploration)\n",
    "file_path = r'D:\\Git repo\\real_estate_listings\\real_estate_project\\data\\raw\\Real_Estate_data.xlsx'\n",
    "\n",
    "# Load all sheets\n",
    "print(\"ğŸ“¥ Loading data for cleaning...\")\n",
    "sheets = {\n",
    "    'active': 'Active_Listings',\n",
    "    'archive': 'Archive', \n",
    "    'clients': 'Client_Database'\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "for short_name, sheet_name in sheets.items():\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    dataframes[short_name] = df\n",
    "    print(f\"âœ… {short_name.title()}: {len(df):,} rows Ã— {df.shape[1]} cols\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Ready for data cleaning!\")\n",
    "print(f\"ğŸ“Š Total records to process: {sum(len(df) for df in dataframes.values()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9cd2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ·ï¸  STANDARDIZING COLUMN NAMES\n",
      "========================================\n",
      "\n",
      "ğŸ“‹ ACTIVE DATASET:\n",
      "   Column name transformations (first 5):\n",
      "     1. 'Property ID' â†’ 'property_id'\n",
      "     2. 'Listing Status' â†’ 'listing_status'\n",
      "     3. 'Listing Type' â†’ 'listing_type'\n",
      "     4. 'Listing Date' â†’ 'listing_date'\n",
      "     5. 'Building / Society' â†’ 'building___society'\n",
      "   âœ… Renamed 28 columns\n",
      "\n",
      "ğŸ“‹ ARCHIVE DATASET:\n",
      "   Column name transformations (first 5):\n",
      "     1. 'Property ID' â†’ 'property_id'\n",
      "     2. 'Listing Status' â†’ 'listing_status'\n",
      "     3. 'Listing Type' â†’ 'listing_type'\n",
      "     4. 'Listing Date' â†’ 'listing_date'\n",
      "     5. 'Closing Date' â†’ 'closing_date'\n",
      "   âœ… Renamed 16 columns\n",
      "\n",
      "ğŸ“‹ CLIENTS DATASET:\n",
      "   Column name transformations (first 5):\n",
      "     1. 'ClientID' â†’ 'clientid'\n",
      "     2. 'Client Name' â†’ 'client_name'\n",
      "     3. 'Client Phone' â†’ 'client_phone'\n",
      "     4. 'Client Email' â†’ 'client_email'\n",
      "     5. 'Looking For' â†’ 'looking_for'\n",
      "   âœ… Renamed 7 columns\n",
      "\n",
      "ğŸ¯ Column standardization complete!\n",
      "ğŸ“Œ All column names are now lowercase, snake_case format\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Standardize Column Names (Create Clean, Consistent Names)\n",
    "print(\"ğŸ·ï¸  STANDARDIZING COLUMN NAMES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def clean_column_name(col_name):\n",
    "    \"\"\"Convert column names to clean, lowercase, snake_case format\"\"\"\n",
    "    # Remove special characters and replace with underscore\n",
    "    clean_name = re.sub(r'[^\\w\\s]', '_', str(col_name))\n",
    "    # Replace spaces and multiple underscores with single underscore\n",
    "    clean_name = re.sub(r'\\s+|_+', '_', clean_name)\n",
    "    # Convert to lowercase and strip leading/trailing underscores\n",
    "    clean_name = clean_name.lower().strip('_')\n",
    "    return clean_name\n",
    "\n",
    "# Store original column names for reference\n",
    "original_columns = {}\n",
    "\n",
    "# Clean column names for each dataframe\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\nğŸ“‹ {name.upper()} DATASET:\")\n",
    "    original_columns[name] = df.columns.tolist()\n",
    "    \n",
    "    # Create mapping of old to new column names\n",
    "    column_mapping = {old_col: clean_column_name(old_col) for old_col in df.columns}\n",
    "    \n",
    "    # Show the transformation for first 5 columns\n",
    "    print(\"   Column name transformations (first 5):\")\n",
    "    for i, (old, new) in enumerate(list(column_mapping.items())[:5]):\n",
    "        print(f\"     {i+1}. '{old}' â†’ '{new}'\")\n",
    "    \n",
    "    # Apply the column renaming\n",
    "    dataframes[name] = df.rename(columns=column_mapping)\n",
    "    \n",
    "    print(f\"   âœ… Renamed {len(column_mapping)} columns\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Column standardization complete!\")\n",
    "print(\"ğŸ“Œ All column names are now lowercase, snake_case format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8791cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’° CLEANING PRICE COLUMNS\n",
      "==============================\n",
      "ğŸ” Identifying price columns in each dataset:\n",
      "   Active: ['asking_price__Ã¢_Â¹', 'monthly_rent__Ã¢_Â¹', 'security_deposit__Ã¢_Â¹', 'price_negotiable']\n",
      "   Archive: ['asking_price__Ã¢_Â¹', 'monthly_rent__Ã¢_Â¹', 'final_price__Ã¢_Â¹']\n",
      "   Clients: []\n",
      "\n",
      "ğŸ§¹ Applying price parsing to all price columns...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Clean and Parse Indian Price Columns\n",
    "print(\"ğŸ’° CLEANING PRICE COLUMNS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# First, let's see which price columns we have after renaming\n",
    "print(\"ğŸ” Identifying price columns in each dataset:\")\n",
    "for name, df in dataframes.items():\n",
    "    price_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['price', 'rent', 'deposit', 'amount'])]\n",
    "    print(f\"   {name.title()}: {price_cols}\")\n",
    "\n",
    "# Indian price parsing function\n",
    "def parse_indian_price(value):\n",
    "    \"\"\"\n",
    "    Parse Indian price formats like '1.25 Cr', '78 Lac', '32,50,000', 'On Request' \n",
    "    Returns float in rupees or NaN for non-numeric values\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert to string and clean\n",
    "    price_str = str(value).lower().strip()\n",
    "    \n",
    "    # Handle non-numeric cases\n",
    "    non_price_terms = ['nan', '', 'on request', 'negotiable', 'slightly', 'call for price']\n",
    "    if price_str in non_price_terms:\n",
    "        return np.nan\n",
    "    \n",
    "    # Remove currency symbols and spaces\n",
    "    price_str = re.sub(r'[â‚¹\\s,]', '', price_str)\n",
    "    \n",
    "    # Parse number with units (Cr, Lac, K)\n",
    "    match = re.match(r'([0-9]*\\.?[0-9]+)([a-z]*)', price_str)\n",
    "    if not match:\n",
    "        return np.nan\n",
    "    \n",
    "    number, unit = match.groups()\n",
    "    try:\n",
    "        number = float(number)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    \n",
    "    # Apply multipliers for Indian units\n",
    "    multipliers = {\n",
    "        'cr': 10000000,    # 1 Crore = 10 Million\n",
    "        'crore': 10000000,\n",
    "        'lac': 100000,     # 1 Lakh = 100 Thousand  \n",
    "        'lakh': 100000,\n",
    "        'l': 100000,       # 'L' often means Lakh\n",
    "        'k': 1000,         # 1 Thousand\n",
    "        '': 1              # No unit = rupees\n",
    "    }\n",
    "    \n",
    "    multiplier = multipliers.get(unit, 1)\n",
    "    return number * multiplier\n",
    "\n",
    "print(f\"\\nğŸ§¹ Applying price parsing to all price columns...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4e47d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ACTIVE DATASET:\n",
      "     ğŸ”§ asking_price__Ã¢_Â¹\n",
      "        Before: ['45500000.0', '28800000.0', '32000000.0']\n",
      "        After:  ['â‚¹45,500,000', 'â‚¹28,800,000', 'â‚¹32,000,000']\n",
      "        Result: 0 text values â†’ NaN, 601 valid prices\n",
      "     ğŸ”§ monthly_rent__Ã¢_Â¹\n",
      "        Before: ['45000.0', '170000.0', '85000.0']\n",
      "        After:  ['â‚¹45,000', 'â‚¹170,000', 'â‚¹85,000']\n",
      "        Result: 0 text values â†’ NaN, 399 valid prices\n",
      "     ğŸ”§ security_deposit__Ã¢_Â¹\n",
      "        Before: ['180000.0', '510000.0', '425000.0']\n",
      "        After:  ['â‚¹180,000', 'â‚¹510,000', 'â‚¹425,000']\n",
      "        Result: 0 text values â†’ NaN, 399 valid prices\n",
      "\n",
      "   ARCHIVE DATASET:\n",
      "     ğŸ”§ asking_price__Ã¢_Â¹\n",
      "        Before: ['19600000.0', '10100000.0', '19300000.0']\n",
      "        After:  ['â‚¹19,600,000', 'â‚¹10,100,000', 'â‚¹19,300,000']\n",
      "        Result: 0 text values â†’ NaN, 514 valid prices\n",
      "     ğŸ”§ monthly_rent__Ã¢_Â¹\n",
      "        Before: ['41000.0', '52000.0', '147000.0']\n",
      "        After:  ['â‚¹41,000', 'â‚¹52,000', 'â‚¹147,000']\n",
      "        Result: 0 text values â†’ NaN, 486 valid prices\n",
      "     ğŸ”§ final_price__Ã¢_Â¹\n",
      "        Before: ['41000', '19146491', '52000']\n",
      "        After:  ['â‚¹41,000', 'â‚¹19,146,491', 'â‚¹52,000']\n",
      "        Result: 0 text values â†’ NaN, 1000 valid prices\n",
      "   Clients: No price columns to clean âœ…\n",
      "\n",
      "   ğŸš© HANDLING NEGOTIABLE FLAGS:\n",
      "     Active price_negotiable before: ['Slightly' 'Yes' 'No']\n",
      "     Active price_negotiable after:  [1 0]\n",
      "\n",
      "âœ… Price column cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 (Continued): Apply the price parsing function\n",
    "# Define the actual price columns (excluding negotiable flag)\n",
    "price_columns_to_clean = [\n",
    "    'asking_price__Ã¢_Â¹', \n",
    "    'monthly_rent__Ã¢_Â¹', \n",
    "    'security_deposit__Ã¢_Â¹',\n",
    "    'final_price__Ã¢_Â¹'\n",
    "]\n",
    "\n",
    "# Apply price parsing to each dataset\n",
    "for name, df in dataframes.items():\n",
    "    if name == 'clients':\n",
    "        print(f\"   {name.title()}: No price columns to clean âœ…\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n   {name.upper()} DATASET:\")\n",
    "    for col in price_columns_to_clean:\n",
    "        if col in df.columns:\n",
    "            # Show sample before cleaning\n",
    "            sample_before = df[col].dropna().head(3).astype(str).tolist()\n",
    "            print(f\"     ğŸ”§ {col}\")\n",
    "            print(f\"        Before: {sample_before}\")\n",
    "            \n",
    "            # Apply price parsing\n",
    "            before_nulls = df[col].isna().sum()\n",
    "            df[col] = df[col].apply(parse_indian_price)\n",
    "            after_nulls = df[col].isna().sum()\n",
    "            new_nulls = after_nulls - before_nulls\n",
    "            \n",
    "            # Show sample after cleaning\n",
    "            sample_after = df[col].dropna().head(3)\n",
    "            formatted_sample = [f\"â‚¹{x:,.0f}\" for x in sample_after] if len(sample_after) > 0 else [\"No valid prices\"]\n",
    "            print(f\"        After:  {formatted_sample}\")\n",
    "            print(f\"        Result: {new_nulls} text values â†’ NaN, {df[col].count()} valid prices\")\n",
    "\n",
    "# Handle price_negotiable separately (it's a boolean flag, not a price)\n",
    "print(f\"\\n   ğŸš© HANDLING NEGOTIABLE FLAGS:\")\n",
    "for name, df in dataframes.items():\n",
    "    if 'price_negotiable' in df.columns:\n",
    "        before_values = df['price_negotiable'].unique()[:5]  # Show first 5 unique values\n",
    "        print(f\"     {name.title()} price_negotiable before: {before_values}\")\n",
    "        \n",
    "        # Convert to boolean (Yes/Slightly/Negotiable = 1, No/other = 0)\n",
    "        df['price_negotiable'] = df['price_negotiable'].astype(str).str.lower().isin(\n",
    "            ['yes', 'y', 'slightly', 'negotiable', '1', 'true']\n",
    "        ).astype(int)\n",
    "        \n",
    "        after_values = df['price_negotiable'].unique()\n",
    "        print(f\"     {name.title()} price_negotiable after:  {after_values}\")\n",
    "\n",
    "print(f\"\\nâœ… Price column cleaning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41a866cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ  CLEANING BHK AND TEXT FIELDS\n",
      "===================================\n",
      "ğŸ”§ Cleaning BHK format:\n",
      "\n",
      "âœ… BHK cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Clean BHK Format and Other Text Fields\n",
    "print(\"ğŸ  CLEANING BHK AND TEXT FIELDS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Function to clean BHK format\n",
    "def clean_bhk(bhk_value):\n",
    "    \"\"\"Convert '3 BHK', '2BHK', 'NaN' to clean integer or NaN\"\"\"\n",
    "    if pd.isna(bhk_value):\n",
    "        return np.nan\n",
    "    \n",
    "    bhk_str = str(bhk_value).upper().strip()\n",
    "    \n",
    "    # Extract number from formats like \"3 BHK\", \"2BHK\", \"3\"\n",
    "    match = re.search(r'(\\d+)', bhk_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Clean BHK columns in Active and Archive datasets\n",
    "print(\"ğŸ”§ Cleaning BHK format:\")\n",
    "for name, df in dataframes.items():\n",
    "    if 'bedrooms__bhk_' in df.columns:\n",
    "        col = 'bedrooms__bhk_'  # Standardized column name\n",
    "        print(f\"\\n   {name.upper()} - {col}:\")\n",
    "        \n",
    "        # Show before cleaning\n",
    "        before_sample = df[col].dropna().head(5).tolist()\n",
    "        before_nulls = df[col].isna().sum()\n",
    "        print(f\"     Before: {before_sample}\")\n",
    "        print(f\"     Null count before: {before_nulls}\")\n",
    "        \n",
    "        # Apply BHK cleaning\n",
    "        df[col] = df[col].apply(clean_bhk)\n",
    "        \n",
    "        # Show after cleaning\n",
    "        after_sample = df[col].dropna().head(5).tolist()\n",
    "        after_nulls = df[col].isna().sum()\n",
    "        print(f\"     After:  {after_sample}\")\n",
    "        print(f\"     Null count after: {after_nulls}\")\n",
    "        print(f\"     Unique BHK values: {sorted(df[col].dropna().unique())}\")\n",
    "\n",
    "print(f\"\\nâœ… BHK cleaning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5045342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DEBUGGING: Actual column names after standardization\n",
      "=======================================================\n",
      "\n",
      "ğŸ“‹ ACTIVE DATASET columns:\n",
      "   BHK-related columns: ['bedrooms__bhk']\n",
      "   First 10 columns: ['property_id', 'listing_status', 'listing_type', 'listing_date', 'building___society', 'area___locality', 'city', 'pincode', 'property_type', 'bedrooms__bhk']\n",
      "   Sample bedrooms__bhk data: ['3 BHK', '3 BHK', '5 BHK']\n",
      "\n",
      "ğŸ“‹ ARCHIVE DATASET columns:\n",
      "   BHK-related columns: ['bedrooms__bhk']\n",
      "   First 10 columns: ['property_id', 'listing_status', 'listing_type', 'listing_date', 'closing_date', 'building___society', 'area___locality', 'city', 'pincode', 'property_type']\n",
      "   Sample bedrooms__bhk data: ['3 BHK', '3 BHK', '2 BHK']\n",
      "\n",
      "ğŸ“‹ CLIENTS DATASET columns:\n",
      "   BHK-related columns: []\n",
      "   First 10 columns: ['clientid', 'client_name', 'client_phone', 'client_email', 'looking_for', 'requirements', 'status']\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what the actual column names are after standardization\n",
    "print(\"ğŸ” DEBUGGING: Actual column names after standardization\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\nğŸ“‹ {name.upper()} DATASET columns:\")\n",
    "    \n",
    "    # Look for BHK-related columns\n",
    "    bhk_cols = [col for col in df.columns if 'bhk' in col.lower() or 'bedroom' in col.lower()]\n",
    "    print(f\"   BHK-related columns: {bhk_cols}\")\n",
    "    \n",
    "    # Show all columns (first 10) to see the actual format\n",
    "    print(f\"   First 10 columns: {list(df.columns[:10])}\")\n",
    "    \n",
    "    # If we find BHK columns, show sample data\n",
    "    for col in bhk_cols:\n",
    "        if col in df.columns:\n",
    "            sample_data = df[col].dropna().head(3).tolist()\n",
    "            print(f\"   Sample {col} data: {sample_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbe04bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ  CLEANING BHK AND TEXT FIELDS\n",
      "===================================\n",
      "ğŸ”§ Cleaning BHK format:\n",
      "\n",
      "   ACTIVE - bedrooms__bhk:\n",
      "     Before: ['3 BHK', '3 BHK', '5 BHK', '3 BHK', '2 BHK']\n",
      "     Null count before: 472\n",
      "     After:  [3.0, 3.0, 5.0, 3.0, 2.0]\n",
      "     Null count after: 472\n",
      "     Unique BHK values: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0)]\n",
      "\n",
      "   ARCHIVE - bedrooms__bhk:\n",
      "     Before: ['3 BHK', '3 BHK', '2 BHK', '3 BHK', '1 BHK']\n",
      "     Null count before: 640\n",
      "     After:  [3.0, 3.0, 2.0, 3.0, 1.0]\n",
      "     Null count after: 640\n",
      "     Unique BHK values: [np.float64(1.0), np.float64(2.0), np.float64(3.0)]\n",
      "\n",
      "âœ… BHK cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Clean BHK Format and Other Text Fields (Corrected)\n",
    "print(\"ğŸ  CLEANING BHK AND TEXT FIELDS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Function to clean BHK format\n",
    "def clean_bhk(bhk_value):\n",
    "    \"\"\"Convert '3 BHK', '2BHK', 'NaN' to clean integer or NaN\"\"\"\n",
    "    if pd.isna(bhk_value):\n",
    "        return np.nan\n",
    "    \n",
    "    bhk_str = str(bhk_value).upper().strip()\n",
    "    \n",
    "    # Extract number from formats like \"3 BHK\", \"2BHK\", \"3\"\n",
    "    match = re.search(r'(\\d+)', bhk_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Clean BHK columns in Active and Archive datasets (using correct column name)\n",
    "print(\"ğŸ”§ Cleaning BHK format:\")\n",
    "for name, df in dataframes.items():\n",
    "    if 'bedrooms__bhk' in df.columns:  # Corrected column name\n",
    "        col = 'bedrooms__bhk'\n",
    "        print(f\"\\n   {name.upper()} - {col}:\")\n",
    "        \n",
    "        # Show before cleaning\n",
    "        before_sample = df[col].dropna().head(5).tolist()\n",
    "        before_nulls = df[col].isna().sum()\n",
    "        print(f\"     Before: {before_sample}\")\n",
    "        print(f\"     Null count before: {before_nulls}\")\n",
    "        \n",
    "        # Apply BHK cleaning\n",
    "        df[col] = df[col].apply(clean_bhk)\n",
    "        \n",
    "        # Show after cleaning\n",
    "        after_sample = df[col].dropna().head(5).tolist()\n",
    "        after_nulls = df[col].isna().sum()\n",
    "        print(f\"     After:  {after_sample}\")\n",
    "        print(f\"     Null count after: {after_nulls}\")\n",
    "        print(f\"     Unique BHK values: {sorted(df[col].dropna().unique())}\")\n",
    "\n",
    "print(f\"\\nâœ… BHK cleaning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f528298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ CLEANING LOCATION AND CATEGORICAL FIELDS\n",
      "=============================================\n",
      "ğŸ”§ Standardizing categorical fields:\n",
      "\n",
      "   ACTIVE DATASET:\n",
      "     ğŸ“‹ city:\n",
      "        Before: ['Mira Bhayandar', 'Mira Bhayandar', 'Mira Bhayandar'] (1 unique)\n",
      "        After:  ['Mira Bhayandar', 'Mira Bhayandar', 'Mira Bhayandar'] (1 unique)\n",
      "     ğŸ“‹ area___locality:\n",
      "        Before: ['Mira Road East', 'Shivar Garden', 'Shanti Nagar'] (10 unique)\n",
      "        After:  ['Mira Road East', 'Shivar Garden', 'Shanti Nagar'] (10 unique)\n",
      "     ğŸ“‹ property_type:\n",
      "        Before: ['Bungalow', 'Apartment', 'Bungalow'] (4 unique)\n",
      "        After:  ['Bungalow', 'Apartment', 'Bungalow'] (4 unique)\n",
      "     ğŸ“‹ furnishing:\n",
      "        Before: ['Semi-Furnished', 'Semi-Furnished', 'Fully Furnished'] (3 unique)\n",
      "        After:  ['Semi-Furnished', 'Semi-Furnished', 'Fully Furnished'] (3 unique)\n",
      "     ğŸ“‹ facing_direction:\n",
      "        Before: ['West', 'North', 'West'] (6 unique)\n",
      "        After:  ['West', 'North', 'West'] (6 unique)\n",
      "     ğŸ“‹ listing_status:\n",
      "        Before: ['Available', 'Available', 'Available'] (1 unique)\n",
      "        After:  ['Available', 'Available', 'Available'] (1 unique)\n",
      "     ğŸ“‹ listing_type:\n",
      "        Before: ['Sale', 'Rent', 'Sale'] (2 unique)\n",
      "        After:  ['Sale', 'Rent', 'Sale'] (2 unique)\n",
      "\n",
      "   ARCHIVE DATASET:\n",
      "     ğŸ“‹ city:\n",
      "        Before: ['Mira Bhayandar', 'Mira Bhayandar', 'Mira Bhayandar'] (1 unique)\n",
      "        After:  ['Mira Bhayandar', 'Mira Bhayandar', 'Mira Bhayandar'] (1 unique)\n",
      "     ğŸ“‹ area___locality:\n",
      "        Before: ['Golden Nest', 'Shanti Nagar', 'Bhayandar East'] (7 unique)\n",
      "        After:  ['Golden Nest', 'Shanti Nagar', 'Bhayandar East'] (7 unique)\n",
      "     ğŸ“‹ property_type:\n",
      "        Before: ['Apartment', 'Office Space', 'Apartment'] (3 unique)\n",
      "        After:  ['Apartment', 'Office Space', 'Apartment'] (3 unique)\n",
      "     ğŸ“‹ listing_status:\n",
      "        Before: ['Rented', 'Sold', 'Rented'] (2 unique)\n",
      "        After:  ['Rented', 'Sold', 'Rented'] (2 unique)\n",
      "     ğŸ“‹ listing_type:\n",
      "        Before: ['Rent', 'Sale', 'Rent'] (2 unique)\n",
      "        After:  ['Rent', 'Sale', 'Rent'] (2 unique)\n",
      "\n",
      "   CLIENTS DATASET:\n",
      "\n",
      "âœ… Location and categorical field cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Standardize Location and Categorical Text Fields\n",
    "print(\"ğŸ“ CLEANING LOCATION AND CATEGORICAL FIELDS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Function to clean and standardize text fields\n",
    "def clean_text_field(text_value):\n",
    "    \"\"\"Standardize text: proper case, remove extra spaces, handle NaN\"\"\"\n",
    "    if pd.isna(text_value):\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert to string, strip whitespace, and proper case\n",
    "    clean_text = str(text_value).strip().title()\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    \n",
    "    return clean_text if clean_text else np.nan\n",
    "\n",
    "# Clean key categorical fields\n",
    "categorical_fields = [\n",
    "    'city', 'area___locality', 'property_type', 'furnishing', \n",
    "    'facing_direction', 'listing_status', 'listing_type'\n",
    "]\n",
    "\n",
    "print(\"ğŸ”§ Standardizing categorical fields:\")\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n   {name.upper()} DATASET:\")\n",
    "    \n",
    "    for field in categorical_fields:\n",
    "        if field in df.columns:\n",
    "            # Show before cleaning (unique values)\n",
    "            before_unique = df[field].nunique() if df[field].count() > 0 else 0\n",
    "            sample_before = df[field].dropna().head(3).tolist()\n",
    "            \n",
    "            # Apply text cleaning\n",
    "            df[field] = df[field].apply(clean_text_field)\n",
    "            \n",
    "            # Show after cleaning\n",
    "            after_unique = df[field].nunique() if df[field].count() > 0 else 0\n",
    "            sample_after = df[field].dropna().head(3).tolist()\n",
    "            \n",
    "            print(f\"     ğŸ“‹ {field}:\")\n",
    "            print(f\"        Before: {sample_before} ({before_unique} unique)\")\n",
    "            print(f\"        After:  {sample_after} ({after_unique} unique)\")\n",
    "\n",
    "print(f\"\\nâœ… Location and categorical field cleaning complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8771d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ SAVING CLEANED DATA AND GENERATING REPORT\n",
      "==================================================\n",
      "ğŸ“ Saving cleaned datasets:\n",
      "   âœ… Active: 1,000 rows â†’ CSV & Parquet saved\n",
      "   âœ… Archive: 1,000 rows â†’ CSV & Parquet saved\n",
      "   âœ… Clients: 1,000 rows â†’ CSV & Parquet saved\n",
      "\n",
      "ğŸ“Š COMPREHENSIVE CLEANING SUMMARY:\n",
      "========================================\n",
      "\n",
      "ğŸ¢ ACTIVE DATASET:\n",
      "   ğŸ“ Dimensions: 1,000 rows Ã— 28 columns\n",
      "   ğŸ’¾ Memory: 0.82 MB\n",
      "   âœ… Complete columns: 24/28\n",
      "   ğŸ“Š Overall completeness: 92.6%\n",
      "   ğŸ”¢ Data types: {dtype('O'): np.int64(12), dtype('int64'): np.int64(11), dtype('float64'): np.int64(4), dtype('<M8[ns]'): np.int64(1)}\n",
      "\n",
      "ğŸ¢ ARCHIVE DATASET:\n",
      "   ğŸ“ Dimensions: 1,000 rows Ã— 16 columns\n",
      "   ğŸ’¾ Memory: 0.51 MB\n",
      "   âœ… Complete columns: 13/16\n",
      "   ğŸ“Š Overall completeness: 89.8%\n",
      "   ğŸ”¢ Data types: {dtype('O'): np.int64(8), dtype('float64'): np.int64(4), dtype('<M8[ns]'): np.int64(2), dtype('int64'): np.int64(2)}\n",
      "\n",
      "ğŸ¢ CLIENTS DATASET:\n",
      "   ğŸ“ Dimensions: 1,000 rows Ã— 7 columns\n",
      "   ğŸ’¾ Memory: 0.51 MB\n",
      "   âœ… Complete columns: 7/7\n",
      "   ğŸ“Š Overall completeness: 100.0%\n",
      "   ğŸ”¢ Data types: {dtype('O'): np.int64(6), dtype('int64'): np.int64(1)}\n",
      "\n",
      "ğŸ¯ CLEANING ACHIEVEMENTS:\n",
      "   âœ… Column names standardized (snake_case format)\n",
      "   âœ… Price columns parsed (â‚¹ values converted to numeric)\n",
      "   âœ… BHK format cleaned (text â†’ integers)\n",
      "   âœ… Categorical fields standardized (proper case)\n",
      "   âœ… Data types optimized for analysis\n",
      "   âœ… Files saved in both CSV and Parquet formats\n",
      "\n",
      "ğŸš€ READY FOR NEXT PHASE: Feature Engineering!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save Cleaned Data and Generate Cleaning Summary Report\n",
    "print(\"ğŸ’¾ SAVING CLEANED DATA AND GENERATING REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create output directory for processed data\n",
    "import os\n",
    "output_dir = r'D:\\Git repo\\real_estate_listings\\real_estate_project\\data\\processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save cleaned dataframes to CSV and Parquet formats\n",
    "print(\"ğŸ“ Saving cleaned datasets:\")\n",
    "for name, df in dataframes.items():\n",
    "    # Save as CSV (human readable)\n",
    "    csv_path = os.path.join(output_dir, f'{name}_cleaned.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Save as Parquet (efficient for data processing)\n",
    "    parquet_path = os.path.join(output_dir, f'{name}_cleaned.parquet')\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    \n",
    "    print(f\"   âœ… {name.title()}: {len(df):,} rows â†’ CSV & Parquet saved\")\n",
    "\n",
    "# Generate comprehensive cleaning summary\n",
    "print(f\"\\nğŸ“Š COMPREHENSIVE CLEANING SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "cleaning_summary = {}\n",
    "for name, df in dataframes.items():\n",
    "    summary = {\n",
    "        'dataset': name.title(),\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'memory_usage_mb': round(df.memory_usage(deep=True).sum() / 1024**2, 2),\n",
    "        'null_percentage': round(df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100, 2),\n",
    "        'complete_columns': (df.isnull().sum() == 0).sum(),\n",
    "        'data_types': dict(df.dtypes.value_counts())\n",
    "    }\n",
    "    cleaning_summary[name] = summary\n",
    "    \n",
    "    print(f\"\\nğŸ¢ {summary['dataset'].upper()} DATASET:\")\n",
    "    print(f\"   ğŸ“ Dimensions: {summary['total_rows']:,} rows Ã— {summary['total_columns']} columns\")\n",
    "    print(f\"   ğŸ’¾ Memory: {summary['memory_usage_mb']} MB\")\n",
    "    print(f\"   âœ… Complete columns: {summary['complete_columns']}/{summary['total_columns']}\")\n",
    "    print(f\"   ğŸ“Š Overall completeness: {100-summary['null_percentage']:.1f}%\")\n",
    "    print(f\"   ğŸ”¢ Data types: {summary['data_types']}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ CLEANING ACHIEVEMENTS:\")\n",
    "print(\"   âœ… Column names standardized (snake_case format)\")\n",
    "print(\"   âœ… Price columns parsed (â‚¹ values converted to numeric)\")\n",
    "print(\"   âœ… BHK format cleaned (text â†’ integers)\")\n",
    "print(\"   âœ… Categorical fields standardized (proper case)\")\n",
    "print(\"   âœ… Data types optimized for analysis\")\n",
    "print(\"   âœ… Files saved in both CSV and Parquet formats\")\n",
    "\n",
    "print(f\"\\nğŸš€ READY FOR NEXT PHASE: Feature Engineering!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a501ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
