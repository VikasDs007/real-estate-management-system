{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60f26dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading data for cleaning...\n",
      "✅ Active: 1,000 rows × 28 cols\n",
      "✅ Archive: 1,000 rows × 16 cols\n",
      "✅ Clients: 1,000 rows × 7 cols\n",
      "\n",
      "🎯 Ready for data cleaning!\n",
      "📊 Total records to process: 3,000\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import Libraries and Setup for Data Cleaning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# File path (same as exploration)\n",
    "file_path = r'D:\\Git repo\\real_estate_listings\\real_estate_project\\data\\raw\\Real_Estate_data.xlsx'\n",
    "\n",
    "# Load all sheets\n",
    "print(\"📥 Loading data for cleaning...\")\n",
    "sheets = {\n",
    "    'active': 'Active_Listings',\n",
    "    'archive': 'Archive', \n",
    "    'clients': 'Client_Database'\n",
    "}\n",
    "\n",
    "dataframes = {}\n",
    "for short_name, sheet_name in sheets.items():\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    dataframes[short_name] = df\n",
    "    print(f\"✅ {short_name.title()}: {len(df):,} rows × {df.shape[1]} cols\")\n",
    "\n",
    "print(f\"\\n🎯 Ready for data cleaning!\")\n",
    "print(f\"📊 Total records to process: {sum(len(df) for df in dataframes.values()):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9cd2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏷️  STANDARDIZING COLUMN NAMES\n",
      "========================================\n",
      "\n",
      "📋 ACTIVE DATASET:\n",
      "   Column name transformations (first 5):\n",
      "     1. 'Property ID' → 'property_id'\n",
      "     2. 'Listing Status' → 'listing_status'\n",
      "     3. 'Listing Type' → 'listing_type'\n",
      "     4. 'Listing Date' → 'listing_date'\n",
      "     5. 'Building / Society' → 'building___society'\n",
      "   ✅ Renamed 28 columns\n",
      "\n",
      "📋 ARCHIVE DATASET:\n",
      "   Column name transformations (first 5):\n",
      "     1. 'Property ID' → 'property_id'\n",
      "     2. 'Listing Status' → 'listing_status'\n",
      "     3. 'Listing Type' → 'listing_type'\n",
      "     4. 'Listing Date' → 'listing_date'\n",
      "     5. 'Closing Date' → 'closing_date'\n",
      "   ✅ Renamed 16 columns\n",
      "\n",
      "📋 CLIENTS DATASET:\n",
      "   Column name transformations (first 5):\n",
      "     1. 'ClientID' → 'clientid'\n",
      "     2. 'Client Name' → 'client_name'\n",
      "     3. 'Client Phone' → 'client_phone'\n",
      "     4. 'Client Email' → 'client_email'\n",
      "     5. 'Looking For' → 'looking_for'\n",
      "   ✅ Renamed 7 columns\n",
      "\n",
      "🎯 Column standardization complete!\n",
      "📌 All column names are now lowercase, snake_case format\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Standardize Column Names (Create Clean, Consistent Names)\n",
    "print(\"🏷️  STANDARDIZING COLUMN NAMES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "def clean_column_name(col_name):\n",
    "    \"\"\"Convert column names to clean, lowercase, snake_case format\"\"\"\n",
    "    # Remove special characters and replace with underscore\n",
    "    clean_name = re.sub(r'[^\\w\\s]', '_', str(col_name))\n",
    "    # Replace spaces and multiple underscores with single underscore\n",
    "    clean_name = re.sub(r'\\s+|_+', '_', clean_name)\n",
    "    # Convert to lowercase and strip leading/trailing underscores\n",
    "    clean_name = clean_name.lower().strip('_')\n",
    "    return clean_name\n",
    "\n",
    "# Store original column names for reference\n",
    "original_columns = {}\n",
    "\n",
    "# Clean column names for each dataframe\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n📋 {name.upper()} DATASET:\")\n",
    "    original_columns[name] = df.columns.tolist()\n",
    "    \n",
    "    # Create mapping of old to new column names\n",
    "    column_mapping = {old_col: clean_column_name(old_col) for old_col in df.columns}\n",
    "    \n",
    "    # Show the transformation for first 5 columns\n",
    "    print(\"   Column name transformations (first 5):\")\n",
    "    for i, (old, new) in enumerate(list(column_mapping.items())[:5]):\n",
    "        print(f\"     {i+1}. '{old}' → '{new}'\")\n",
    "    \n",
    "    # Apply the column renaming\n",
    "    dataframes[name] = df.rename(columns=column_mapping)\n",
    "    \n",
    "    print(f\"   ✅ Renamed {len(column_mapping)} columns\")\n",
    "\n",
    "print(f\"\\n🎯 Column standardization complete!\")\n",
    "print(\"📌 All column names are now lowercase, snake_case format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8791cc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💰 CLEANING PRICE COLUMNS\n",
      "==============================\n",
      "🔍 Identifying price columns in each dataset:\n",
      "   Active: ['asking_price__â_¹', 'monthly_rent__â_¹', 'security_deposit__â_¹', 'price_negotiable']\n",
      "   Archive: ['asking_price__â_¹', 'monthly_rent__â_¹', 'final_price__â_¹']\n",
      "   Clients: []\n",
      "\n",
      "🧹 Applying price parsing to all price columns...\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Clean and Parse Indian Price Columns\n",
    "print(\"💰 CLEANING PRICE COLUMNS\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# First, let's see which price columns we have after renaming\n",
    "print(\"🔍 Identifying price columns in each dataset:\")\n",
    "for name, df in dataframes.items():\n",
    "    price_cols = [col for col in df.columns if any(keyword in col.lower() \n",
    "                  for keyword in ['price', 'rent', 'deposit', 'amount'])]\n",
    "    print(f\"   {name.title()}: {price_cols}\")\n",
    "\n",
    "# Indian price parsing function\n",
    "def parse_indian_price(value):\n",
    "    \"\"\"\n",
    "    Parse Indian price formats like '1.25 Cr', '78 Lac', '32,50,000', 'On Request' \n",
    "    Returns float in rupees or NaN for non-numeric values\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert to string and clean\n",
    "    price_str = str(value).lower().strip()\n",
    "    \n",
    "    # Handle non-numeric cases\n",
    "    non_price_terms = ['nan', '', 'on request', 'negotiable', 'slightly', 'call for price']\n",
    "    if price_str in non_price_terms:\n",
    "        return np.nan\n",
    "    \n",
    "    # Remove currency symbols and spaces\n",
    "    price_str = re.sub(r'[₹\\s,]', '', price_str)\n",
    "    \n",
    "    # Parse number with units (Cr, Lac, K)\n",
    "    match = re.match(r'([0-9]*\\.?[0-9]+)([a-z]*)', price_str)\n",
    "    if not match:\n",
    "        return np.nan\n",
    "    \n",
    "    number, unit = match.groups()\n",
    "    try:\n",
    "        number = float(number)\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "    \n",
    "    # Apply multipliers for Indian units\n",
    "    multipliers = {\n",
    "        'cr': 10000000,    # 1 Crore = 10 Million\n",
    "        'crore': 10000000,\n",
    "        'lac': 100000,     # 1 Lakh = 100 Thousand  \n",
    "        'lakh': 100000,\n",
    "        'l': 100000,       # 'L' often means Lakh\n",
    "        'k': 1000,         # 1 Thousand\n",
    "        '': 1              # No unit = rupees\n",
    "    }\n",
    "    \n",
    "    multiplier = multipliers.get(unit, 1)\n",
    "    return number * multiplier\n",
    "\n",
    "print(f\"\\n🧹 Applying price parsing to all price columns...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e4e47d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "   ACTIVE DATASET:\n",
      "     🔧 asking_price__â_¹\n",
      "        Before: ['45500000.0', '28800000.0', '32000000.0']\n",
      "        After:  ['₹45,500,000', '₹28,800,000', '₹32,000,000']\n",
      "        Result: 0 text values → NaN, 601 valid prices\n",
      "     🔧 monthly_rent__â_¹\n",
      "        Before: ['45000.0', '170000.0', '85000.0']\n",
      "        After:  ['₹45,000', '₹170,000', '₹85,000']\n",
      "        Result: 0 text values → NaN, 399 valid prices\n",
      "     🔧 security_deposit__â_¹\n",
      "        Before: ['180000.0', '510000.0', '425000.0']\n",
      "        After:  ['₹180,000', '₹510,000', '₹425,000']\n",
      "        Result: 0 text values → NaN, 399 valid prices\n",
      "\n",
      "   ARCHIVE DATASET:\n",
      "     🔧 asking_price__â_¹\n",
      "        Before: ['19600000.0', '10100000.0', '19300000.0']\n",
      "        After:  ['₹19,600,000', '₹10,100,000', '₹19,300,000']\n",
      "        Result: 0 text values → NaN, 514 valid prices\n",
      "     🔧 monthly_rent__â_¹\n",
      "        Before: ['41000.0', '52000.0', '147000.0']\n",
      "        After:  ['₹41,000', '₹52,000', '₹147,000']\n",
      "        Result: 0 text values → NaN, 486 valid prices\n",
      "     🔧 final_price__â_¹\n",
      "        Before: ['41000', '19146491', '52000']\n",
      "        After:  ['₹41,000', '₹19,146,491', '₹52,000']\n",
      "        Result: 0 text values → NaN, 1000 valid prices\n",
      "   Clients: No price columns to clean ✅\n",
      "\n",
      "   🚩 HANDLING NEGOTIABLE FLAGS:\n",
      "     Active price_negotiable before: ['Slightly' 'Yes' 'No']\n",
      "     Active price_negotiable after:  [1 0]\n",
      "\n",
      "✅ Price column cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 (Continued): Apply the price parsing function\n",
    "# Define the actual price columns (excluding negotiable flag)\n",
    "price_columns_to_clean = [\n",
    "    'asking_price__â_¹', \n",
    "    'monthly_rent__â_¹', \n",
    "    'security_deposit__â_¹',\n",
    "    'final_price__â_¹'\n",
    "]\n",
    "\n",
    "# Apply price parsing to each dataset\n",
    "for name, df in dataframes.items():\n",
    "    if name == 'clients':\n",
    "        print(f\"   {name.title()}: No price columns to clean ✅\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n   {name.upper()} DATASET:\")\n",
    "    for col in price_columns_to_clean:\n",
    "        if col in df.columns:\n",
    "            # Show sample before cleaning\n",
    "            sample_before = df[col].dropna().head(3).astype(str).tolist()\n",
    "            print(f\"     🔧 {col}\")\n",
    "            print(f\"        Before: {sample_before}\")\n",
    "            \n",
    "            # Apply price parsing\n",
    "            before_nulls = df[col].isna().sum()\n",
    "            df[col] = df[col].apply(parse_indian_price)\n",
    "            after_nulls = df[col].isna().sum()\n",
    "            new_nulls = after_nulls - before_nulls\n",
    "            \n",
    "            # Show sample after cleaning\n",
    "            sample_after = df[col].dropna().head(3)\n",
    "            formatted_sample = [f\"₹{x:,.0f}\" for x in sample_after] if len(sample_after) > 0 else [\"No valid prices\"]\n",
    "            print(f\"        After:  {formatted_sample}\")\n",
    "            print(f\"        Result: {new_nulls} text values → NaN, {df[col].count()} valid prices\")\n",
    "\n",
    "# Handle price_negotiable separately (it's a boolean flag, not a price)\n",
    "print(f\"\\n   🚩 HANDLING NEGOTIABLE FLAGS:\")\n",
    "for name, df in dataframes.items():\n",
    "    if 'price_negotiable' in df.columns:\n",
    "        before_values = df['price_negotiable'].unique()[:5]  # Show first 5 unique values\n",
    "        print(f\"     {name.title()} price_negotiable before: {before_values}\")\n",
    "        \n",
    "        # Convert to boolean (Yes/Slightly/Negotiable = 1, No/other = 0)\n",
    "        df['price_negotiable'] = df['price_negotiable'].astype(str).str.lower().isin(\n",
    "            ['yes', 'y', 'slightly', 'negotiable', '1', 'true']\n",
    "        ).astype(int)\n",
    "        \n",
    "        after_values = df['price_negotiable'].unique()\n",
    "        print(f\"     {name.title()} price_negotiable after:  {after_values}\")\n",
    "\n",
    "print(f\"\\n✅ Price column cleaning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41a866cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏠 CLEANING BHK AND TEXT FIELDS\n",
      "===================================\n",
      "🔧 Cleaning BHK format:\n",
      "\n",
      "✅ BHK cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Clean BHK Format and Other Text Fields\n",
    "print(\"🏠 CLEANING BHK AND TEXT FIELDS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Function to clean BHK format\n",
    "def clean_bhk(bhk_value):\n",
    "    \"\"\"Convert '3 BHK', '2BHK', 'NaN' to clean integer or NaN\"\"\"\n",
    "    if pd.isna(bhk_value):\n",
    "        return np.nan\n",
    "    \n",
    "    bhk_str = str(bhk_value).upper().strip()\n",
    "    \n",
    "    # Extract number from formats like \"3 BHK\", \"2BHK\", \"3\"\n",
    "    match = re.search(r'(\\d+)', bhk_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Clean BHK columns in Active and Archive datasets\n",
    "print(\"🔧 Cleaning BHK format:\")\n",
    "for name, df in dataframes.items():\n",
    "    if 'bedrooms__bhk_' in df.columns:\n",
    "        col = 'bedrooms__bhk_'  # Standardized column name\n",
    "        print(f\"\\n   {name.upper()} - {col}:\")\n",
    "        \n",
    "        # Show before cleaning\n",
    "        before_sample = df[col].dropna().head(5).tolist()\n",
    "        before_nulls = df[col].isna().sum()\n",
    "        print(f\"     Before: {before_sample}\")\n",
    "        print(f\"     Null count before: {before_nulls}\")\n",
    "        \n",
    "        # Apply BHK cleaning\n",
    "        df[col] = df[col].apply(clean_bhk)\n",
    "        \n",
    "        # Show after cleaning\n",
    "        after_sample = df[col].dropna().head(5).tolist()\n",
    "        after_nulls = df[col].isna().sum()\n",
    "        print(f\"     After:  {after_sample}\")\n",
    "        print(f\"     Null count after: {after_nulls}\")\n",
    "        print(f\"     Unique BHK values: {sorted(df[col].dropna().unique())}\")\n",
    "\n",
    "print(f\"\\n✅ BHK cleaning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5045342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 DEBUGGING: Actual column names after standardization\n",
      "=======================================================\n",
      "\n",
      "📋 ACTIVE DATASET columns:\n",
      "   BHK-related columns: ['bedrooms__bhk']\n",
      "   First 10 columns: ['property_id', 'listing_status', 'listing_type', 'listing_date', 'building___society', 'area___locality', 'city', 'pincode', 'property_type', 'bedrooms__bhk']\n",
      "   Sample bedrooms__bhk data: ['3 BHK', '3 BHK', '5 BHK']\n",
      "\n",
      "📋 ARCHIVE DATASET columns:\n",
      "   BHK-related columns: ['bedrooms__bhk']\n",
      "   First 10 columns: ['property_id', 'listing_status', 'listing_type', 'listing_date', 'closing_date', 'building___society', 'area___locality', 'city', 'pincode', 'property_type']\n",
      "   Sample bedrooms__bhk data: ['3 BHK', '3 BHK', '2 BHK']\n",
      "\n",
      "📋 CLIENTS DATASET columns:\n",
      "   BHK-related columns: []\n",
      "   First 10 columns: ['clientid', 'client_name', 'client_phone', 'client_email', 'looking_for', 'requirements', 'status']\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check what the actual column names are after standardization\n",
    "print(\"🔍 DEBUGGING: Actual column names after standardization\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n📋 {name.upper()} DATASET columns:\")\n",
    "    \n",
    "    # Look for BHK-related columns\n",
    "    bhk_cols = [col for col in df.columns if 'bhk' in col.lower() or 'bedroom' in col.lower()]\n",
    "    print(f\"   BHK-related columns: {bhk_cols}\")\n",
    "    \n",
    "    # Show all columns (first 10) to see the actual format\n",
    "    print(f\"   First 10 columns: {list(df.columns[:10])}\")\n",
    "    \n",
    "    # If we find BHK columns, show sample data\n",
    "    for col in bhk_cols:\n",
    "        if col in df.columns:\n",
    "            sample_data = df[col].dropna().head(3).tolist()\n",
    "            print(f\"   Sample {col} data: {sample_data}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbe04bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏠 CLEANING BHK AND TEXT FIELDS\n",
      "===================================\n",
      "🔧 Cleaning BHK format:\n",
      "\n",
      "   ACTIVE - bedrooms__bhk:\n",
      "     Before: ['3 BHK', '3 BHK', '5 BHK', '3 BHK', '2 BHK']\n",
      "     Null count before: 472\n",
      "     After:  [3.0, 3.0, 5.0, 3.0, 2.0]\n",
      "     Null count after: 472\n",
      "     Unique BHK values: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0)]\n",
      "\n",
      "   ARCHIVE - bedrooms__bhk:\n",
      "     Before: ['3 BHK', '3 BHK', '2 BHK', '3 BHK', '1 BHK']\n",
      "     Null count before: 640\n",
      "     After:  [3.0, 3.0, 2.0, 3.0, 1.0]\n",
      "     Null count after: 640\n",
      "     Unique BHK values: [np.float64(1.0), np.float64(2.0), np.float64(3.0)]\n",
      "\n",
      "✅ BHK cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Clean BHK Format and Other Text Fields (Corrected)\n",
    "print(\"🏠 CLEANING BHK AND TEXT FIELDS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Function to clean BHK format\n",
    "def clean_bhk(bhk_value):\n",
    "    \"\"\"Convert '3 BHK', '2BHK', 'NaN' to clean integer or NaN\"\"\"\n",
    "    if pd.isna(bhk_value):\n",
    "        return np.nan\n",
    "    \n",
    "    bhk_str = str(bhk_value).upper().strip()\n",
    "    \n",
    "    # Extract number from formats like \"3 BHK\", \"2BHK\", \"3\"\n",
    "    match = re.search(r'(\\d+)', bhk_str)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "# Clean BHK columns in Active and Archive datasets (using correct column name)\n",
    "print(\"🔧 Cleaning BHK format:\")\n",
    "for name, df in dataframes.items():\n",
    "    if 'bedrooms__bhk' in df.columns:  # Corrected column name\n",
    "        col = 'bedrooms__bhk'\n",
    "        print(f\"\\n   {name.upper()} - {col}:\")\n",
    "        \n",
    "        # Show before cleaning\n",
    "        before_sample = df[col].dropna().head(5).tolist()\n",
    "        before_nulls = df[col].isna().sum()\n",
    "        print(f\"     Before: {before_sample}\")\n",
    "        print(f\"     Null count before: {before_nulls}\")\n",
    "        \n",
    "        # Apply BHK cleaning\n",
    "        df[col] = df[col].apply(clean_bhk)\n",
    "        \n",
    "        # Show after cleaning\n",
    "        after_sample = df[col].dropna().head(5).tolist()\n",
    "        after_nulls = df[col].isna().sum()\n",
    "        print(f\"     After:  {after_sample}\")\n",
    "        print(f\"     Null count after: {after_nulls}\")\n",
    "        print(f\"     Unique BHK values: {sorted(df[col].dropna().unique())}\")\n",
    "\n",
    "print(f\"\\n✅ BHK cleaning complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f528298b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📍 CLEANING LOCATION AND CATEGORICAL FIELDS\n",
      "=============================================\n",
      "🔧 Standardizing categorical fields:\n",
      "\n",
      "   ACTIVE DATASET:\n",
      "     📋 city:\n",
      "        Before: ['Mira Bhayandar', 'Mira Bhayandar', 'Mira Bhayandar'] (1 unique)\n",
      "        After:  ['Mira Bhayandar', 'Mira Bhayandar', 'Mira Bhayandar'] (1 unique)\n",
      "     📋 area___locality:\n",
      "        Before: ['Mira Road East', 'Shivar Garden', 'Shanti Nagar'] (10 unique)\n",
      "        After:  ['Mira Road East', 'Shivar Garden', 'Shanti Nagar'] (10 unique)\n",
      "     📋 property_type:\n",
      "        Before: ['Bungalow', 'Apartment', 'Bungalow'] (4 unique)\n",
      "        After:  ['Bungalow', 'Apartment', 'Bungalow'] (4 unique)\n",
      "     📋 furnishing:\n",
      "        Before: ['Semi-Furnished', 'Semi-Furnished', 'Fully Furnished'] (3 unique)\n",
      "        After:  ['Semi-Furnished', 'Semi-Furnished', 'Fully Furnished'] (3 unique)\n",
      "     📋 facing_direction:\n",
      "        Before: ['West', 'North', 'West'] (6 unique)\n",
      "        After:  ['West', 'North', 'West'] (6 unique)\n",
      "     📋 listing_status:\n",
      "        Before: ['Available', 'Available', 'Available'] (1 unique)\n",
      "        After:  ['Available', 'Available', 'Available'] (1 unique)\n",
      "     📋 listing_type:\n",
      "        Before: ['Sale', 'Rent', 'Sale'] (2 unique)\n",
      "        After:  ['Sale', 'Rent', 'Sale'] (2 unique)\n",
      "\n",
      "   ARCHIVE DATASET:\n",
      "     📋 city:\n",
      "        Before: ['Mira Bhayandar', 'Mira Bhayandar', 'Mira Bhayandar'] (1 unique)\n",
      "        After:  ['Mira Bhayandar', 'Mira Bhayandar', 'Mira Bhayandar'] (1 unique)\n",
      "     📋 area___locality:\n",
      "        Before: ['Golden Nest', 'Shanti Nagar', 'Bhayandar East'] (7 unique)\n",
      "        After:  ['Golden Nest', 'Shanti Nagar', 'Bhayandar East'] (7 unique)\n",
      "     📋 property_type:\n",
      "        Before: ['Apartment', 'Office Space', 'Apartment'] (3 unique)\n",
      "        After:  ['Apartment', 'Office Space', 'Apartment'] (3 unique)\n",
      "     📋 listing_status:\n",
      "        Before: ['Rented', 'Sold', 'Rented'] (2 unique)\n",
      "        After:  ['Rented', 'Sold', 'Rented'] (2 unique)\n",
      "     📋 listing_type:\n",
      "        Before: ['Rent', 'Sale', 'Rent'] (2 unique)\n",
      "        After:  ['Rent', 'Sale', 'Rent'] (2 unique)\n",
      "\n",
      "   CLIENTS DATASET:\n",
      "\n",
      "✅ Location and categorical field cleaning complete!\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Standardize Location and Categorical Text Fields\n",
    "print(\"📍 CLEANING LOCATION AND CATEGORICAL FIELDS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Function to clean and standardize text fields\n",
    "def clean_text_field(text_value):\n",
    "    \"\"\"Standardize text: proper case, remove extra spaces, handle NaN\"\"\"\n",
    "    if pd.isna(text_value):\n",
    "        return np.nan\n",
    "    \n",
    "    # Convert to string, strip whitespace, and proper case\n",
    "    clean_text = str(text_value).strip().title()\n",
    "    \n",
    "    # Replace multiple spaces with single space\n",
    "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "    \n",
    "    return clean_text if clean_text else np.nan\n",
    "\n",
    "# Clean key categorical fields\n",
    "categorical_fields = [\n",
    "    'city', 'area___locality', 'property_type', 'furnishing', \n",
    "    'facing_direction', 'listing_status', 'listing_type'\n",
    "]\n",
    "\n",
    "print(\"🔧 Standardizing categorical fields:\")\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\n   {name.upper()} DATASET:\")\n",
    "    \n",
    "    for field in categorical_fields:\n",
    "        if field in df.columns:\n",
    "            # Show before cleaning (unique values)\n",
    "            before_unique = df[field].nunique() if df[field].count() > 0 else 0\n",
    "            sample_before = df[field].dropna().head(3).tolist()\n",
    "            \n",
    "            # Apply text cleaning\n",
    "            df[field] = df[field].apply(clean_text_field)\n",
    "            \n",
    "            # Show after cleaning\n",
    "            after_unique = df[field].nunique() if df[field].count() > 0 else 0\n",
    "            sample_after = df[field].dropna().head(3).tolist()\n",
    "            \n",
    "            print(f\"     📋 {field}:\")\n",
    "            print(f\"        Before: {sample_before} ({before_unique} unique)\")\n",
    "            print(f\"        After:  {sample_after} ({after_unique} unique)\")\n",
    "\n",
    "print(f\"\\n✅ Location and categorical field cleaning complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8771d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 SAVING CLEANED DATA AND GENERATING REPORT\n",
      "==================================================\n",
      "📁 Saving cleaned datasets:\n",
      "   ✅ Active: 1,000 rows → CSV & Parquet saved\n",
      "   ✅ Archive: 1,000 rows → CSV & Parquet saved\n",
      "   ✅ Clients: 1,000 rows → CSV & Parquet saved\n",
      "\n",
      "📊 COMPREHENSIVE CLEANING SUMMARY:\n",
      "========================================\n",
      "\n",
      "🏢 ACTIVE DATASET:\n",
      "   📏 Dimensions: 1,000 rows × 28 columns\n",
      "   💾 Memory: 0.82 MB\n",
      "   ✅ Complete columns: 24/28\n",
      "   📊 Overall completeness: 92.6%\n",
      "   🔢 Data types: {dtype('O'): np.int64(12), dtype('int64'): np.int64(11), dtype('float64'): np.int64(4), dtype('<M8[ns]'): np.int64(1)}\n",
      "\n",
      "🏢 ARCHIVE DATASET:\n",
      "   📏 Dimensions: 1,000 rows × 16 columns\n",
      "   💾 Memory: 0.51 MB\n",
      "   ✅ Complete columns: 13/16\n",
      "   📊 Overall completeness: 89.8%\n",
      "   🔢 Data types: {dtype('O'): np.int64(8), dtype('float64'): np.int64(4), dtype('<M8[ns]'): np.int64(2), dtype('int64'): np.int64(2)}\n",
      "\n",
      "🏢 CLIENTS DATASET:\n",
      "   📏 Dimensions: 1,000 rows × 7 columns\n",
      "   💾 Memory: 0.51 MB\n",
      "   ✅ Complete columns: 7/7\n",
      "   📊 Overall completeness: 100.0%\n",
      "   🔢 Data types: {dtype('O'): np.int64(6), dtype('int64'): np.int64(1)}\n",
      "\n",
      "🎯 CLEANING ACHIEVEMENTS:\n",
      "   ✅ Column names standardized (snake_case format)\n",
      "   ✅ Price columns parsed (₹ values converted to numeric)\n",
      "   ✅ BHK format cleaned (text → integers)\n",
      "   ✅ Categorical fields standardized (proper case)\n",
      "   ✅ Data types optimized for analysis\n",
      "   ✅ Files saved in both CSV and Parquet formats\n",
      "\n",
      "🚀 READY FOR NEXT PHASE: Feature Engineering!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Save Cleaned Data and Generate Cleaning Summary Report\n",
    "print(\"💾 SAVING CLEANED DATA AND GENERATING REPORT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create output directory for processed data\n",
    "import os\n",
    "output_dir = r'D:\\Git repo\\real_estate_listings\\real_estate_project\\data\\processed'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save cleaned dataframes to CSV and Parquet formats\n",
    "print(\"📁 Saving cleaned datasets:\")\n",
    "for name, df in dataframes.items():\n",
    "    # Save as CSV (human readable)\n",
    "    csv_path = os.path.join(output_dir, f'{name}_cleaned.csv')\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    \n",
    "    # Save as Parquet (efficient for data processing)\n",
    "    parquet_path = os.path.join(output_dir, f'{name}_cleaned.parquet')\n",
    "    df.to_parquet(parquet_path, index=False)\n",
    "    \n",
    "    print(f\"   ✅ {name.title()}: {len(df):,} rows → CSV & Parquet saved\")\n",
    "\n",
    "# Generate comprehensive cleaning summary\n",
    "print(f\"\\n📊 COMPREHENSIVE CLEANING SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "cleaning_summary = {}\n",
    "for name, df in dataframes.items():\n",
    "    summary = {\n",
    "        'dataset': name.title(),\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'memory_usage_mb': round(df.memory_usage(deep=True).sum() / 1024**2, 2),\n",
    "        'null_percentage': round(df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100, 2),\n",
    "        'complete_columns': (df.isnull().sum() == 0).sum(),\n",
    "        'data_types': dict(df.dtypes.value_counts())\n",
    "    }\n",
    "    cleaning_summary[name] = summary\n",
    "    \n",
    "    print(f\"\\n🏢 {summary['dataset'].upper()} DATASET:\")\n",
    "    print(f\"   📏 Dimensions: {summary['total_rows']:,} rows × {summary['total_columns']} columns\")\n",
    "    print(f\"   💾 Memory: {summary['memory_usage_mb']} MB\")\n",
    "    print(f\"   ✅ Complete columns: {summary['complete_columns']}/{summary['total_columns']}\")\n",
    "    print(f\"   📊 Overall completeness: {100-summary['null_percentage']:.1f}%\")\n",
    "    print(f\"   🔢 Data types: {summary['data_types']}\")\n",
    "\n",
    "print(f\"\\n🎯 CLEANING ACHIEVEMENTS:\")\n",
    "print(\"   ✅ Column names standardized (snake_case format)\")\n",
    "print(\"   ✅ Price columns parsed (₹ values converted to numeric)\")\n",
    "print(\"   ✅ BHK format cleaned (text → integers)\")\n",
    "print(\"   ✅ Categorical fields standardized (proper case)\")\n",
    "print(\"   ✅ Data types optimized for analysis\")\n",
    "print(\"   ✅ Files saved in both CSV and Parquet formats\")\n",
    "\n",
    "print(f\"\\n🚀 READY FOR NEXT PHASE: Feature Engineering!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a501ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
